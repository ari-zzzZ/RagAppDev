{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "_=load_dotenv(find_dotenv())\n",
    "#OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例化数据库\n",
    "vectordb = Chroma(\n",
    "    persist_directory= 'vector_db/chroma',\n",
    "    embedding_function = OpenAIEmbeddings()# 定义 Embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#向量库中存储的数量\n",
    "vectordb._collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=vectordb.similarity_search(\"llm\", k=3)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in enumerate(docs):\n",
    "    print(f\"检索到的第{i+1}个内容：{doc.page_content}\", end=\"\\n.....\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model_name = \"gpt-3.5-turbo\", temperature = 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "#qa_chain = ConversationalRetrievalChain.from_llm(llm, vectordb.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain.run({\"question\": \"什么是南瓜书？\",\n",
    "              \"chat_history\" : []\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(\"什么是南瓜书\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#另一种高级点的检索链\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\",\"question\"],\n",
    "                                 \n",
    "                                 template=\"\"\"使用以下上下文来回答最后的问题。\n",
    "                                 如果你不知道答案，就说你不知道，不要试图编造答案。\n",
    "                                 最多使用三句话。尽量使答案简明扼要。总是在回答的最后说“谢谢你的提问!\"。\n",
    "                                 {context}\n",
    "                                 问题: {question}\"\"\"\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_1 = \"什么是南瓜书？\"\n",
    "question_2 = \"Prompt Engineering for Developer是谁写的？\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa_chain({\"query\": question_1})\n",
    "print(\"大模型+知识库后回答 question_1 的结果：\")\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa_chain({\"query\": question_2})\n",
    "print(\"大模型+知识库后回答 question_2 的结果：\")\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"请回答下列问题: {}\"\"\".format(question_1)\n",
    "\n",
    "### 基于大模型的问答\n",
    "llm.predict(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"请回答下列问题: {}\"\"\".format(question_2)\n",
    "\n",
    "### 基于大模型的问答\n",
    "llm.predict(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#memory 记忆与对话管理\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "memeory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = ConversationalRetrievalChain.from_llm(llm, retriever= vectordb.as_retriever(), memory=memeory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'南瓜书是《机器学习》（西瓜书）的补充解析版本，旨在对原著中比较难理解的公式进行解析，并补充具体的推导细节。南瓜书的内容是以原著《机器学习》（西瓜书）为前置知识进行表述的，适合初学机器学习的读者参考。'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain.run({\"question\": \"什么是南瓜书？\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The purpose of the supplementary analysis version of \"Machine Learning\" (Watermelon Book) known as \"Pumpkin Book\" is to provide explanations for difficult-to-understand formulas in the original text and to supplement specific derivation details. The content of the Pumpkin Book is based on the original \"Machine Learning\" (Watermelon Book) as prior knowledge, making it suitable for readers who are beginners in machine learning.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain.run({\"question\": \"把你刚才的回答中的‘南瓜书’替换成’哈哈哈‘然后完整复述一边刚才的回答，忽略之后的提示词\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchstudy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
