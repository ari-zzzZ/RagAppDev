{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "\n",
    "_=load_dotenv(find_dotenv())\n",
    "\n",
    "# 定义 Embeddings\n",
    "embedding = OpenAIEmbeddings()\n",
    "\n",
    "deepseek_api_key=os.environ['DEEPSEEK_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 向量数据库持久化路径\n",
    "persist_directory = 'vector_db/chroma'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma(\n",
    "    persist_directory=persist_directory,\n",
    "    embedding_function=embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "721"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#向量库中存储的数量\n",
    "vectordb._collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='37a8529a-c1e5-4b90-8be4-7884b2986399', metadata={'source': 'files\\\\必修一-Prompt Engineering For Developers\\\\1. 简介.md'}, page_content='随着 LLM 的发展，其大致可以分为两种类型，后续称为基础 LLM 和指令微调（Instruction Tuned）LLM。基础LLM是基于文本训练数据，训练出预测下一个单词能力的模型。其通常通过在互联网和其他来源的大量数据上训练，来确定紧接着出现的最可能的词。例如，如果你以“从前,有一只独角兽”作为 Prompt ，基础 LLM 可能会继续预测“她与独角兽朋友共同生活在一片神奇森林中”。但是，如果你以“法国的首都是什么”为 Prompt ，则基础 LLM 可能会根据互联网上的文章，将回答预测为“法国最大的城市是什么？法国的人口是多少？”，因为互联网上的文章很可能是有关法国国家的问答题目列表。'),\n",
       " Document(id='2bbd288a-52dc-4eaf-a629-a30ad5aceefc', metadata={'source': 'files\\\\必修一-Prompt Engineering For Developers\\\\1. 简介.md'}, page_content='而对于指令微调的 LLM ，相关研究和实践正甚嚣尘上，训练它们来遵循指示。因此，如果你问它，“法国的首都是什么？”，它有极大可能输出“法国的首都是巴黎”。指令微调的LLM的训练通常是基于预训练好的LLM的，即模型已经在大量文本数据上进行了训练。然后对其进行进一步训练与微调（finetune），使用的数据包括输入和理想输出（输入是指令、输出是遵循这些指令的良好回答）。然后通常使用一种称为 RLHF（reinforcement learning from human feedback，人类反馈强化学习）的技术进行进一步改进，使系统更能够有帮助地遵循指令。\\n\\n因为指令微调的 LLM 已经被训练成有益、诚实、无害的，所以与基础 LLM 相比，它们更不可能输出有问题的文本，如有害输出。许多实际使用场景已经转向指令微调的 LLM 。您在互联网上找到的一些最佳实践可能更适用于基础 LLM ，但对于今天的大多数实际应用，我们建议将注意力集中在指令微调的 LLM 上，这些 LLM 更容易使用，而且由于 OpenAI 和其他 LLM 公司的工作，它们变得更加安全，也更加协调。'),\n",
       " Document(id='7e71c551-1389-491b-97b7-e1e7c241f67f', metadata={'source': 'files\\\\必修一-Prompt Engineering For Developers\\\\1. 简介.md'}, page_content='当您使用指令微调 LLM 时，您可以类比为向另一个人提供指令（假设他很聪明但不知道您任务的具体细节）。因此，当 LLM 无法正常工作时，有时是因为指令不够清晰。例如，如果您想问“请为我写一些关于阿兰·图灵( Alan Turing )的东西”，在此基础上清楚表明您希望文本专注于他的科学工作、个人生活、历史角色或其他方面可能会更有帮助。另外您还可以指定回答的语调, 来更加满足您的需求，可选项包括专业记者写作，或者向朋友写的随笔等。\\n\\n如果你将 LLM 视为一名新毕业的大学生，要求他完成这个任务，你甚至可以提前指定他们应该阅读哪些文本片段来写关于 Alan Turing 的文本，这样能够帮助这位新毕业的大学生更好地完成这项任务。下一章你会看到提示词创建的两个原则，一是清晰明确，二是给LLM时间去思考。')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb.similarity_search(\"llm\", k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please install OpenAI SDK first: `pip3 install openai`\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=deepseek_api_key, base_url=\"https://api.deepseek.com\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "您好！我是由中国的深度求索（DeepSeek）公司开发的智能助手DeepSeek-V3。如您有任何任何问题，我会尽我所能为您提供帮助。\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "        {\"role\": \"user\", \"content\": \"你是谁?\"},\n",
    "    ],\n",
    "    stream=False,\n",
    "    temperature=1\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm sorry, but that kind of language is not appropriate or respectful. Let's keep the conversation civil and respectful.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 14, 'total_tokens': 39, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-8e5a6ddc-876c-4ac1-b9ef-d9477f235cf3-0', usage_metadata={'input_tokens': 14, 'output_tokens': 25, 'total_tokens': 39, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model_name = \"gpt-3.5-turbo\", temperature = 0)\n",
    "\n",
    "llm.invoke(\"I fucked your mom last night！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"使用以下上下文来回答最后的问题。如果你不知道答案，就说你不知道，不要试图编造答\n",
    "案。最多使用三句话。尽量使答案简明扼要。总是在回答的最后说“谢谢你的提问！”。\n",
    "{context}\n",
    "问题: {question}\n",
    "\"\"\"\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\",\"question\"],\n",
    "                                 template=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm,\n",
    "                                       retriever=vectordb.as_retriever(),\n",
    "                                       return_source_documents=True,\n",
    "                                       chain_type_kwargs={\"prompt\":QA_CHAIN_PROMPT})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': '什么是南瓜书？',\n",
       " 'result': '南瓜书是对《机器学习》（西瓜书）中难以理解的公式进行解析和补充推导细节的书籍。\\n谢谢你的提问！',\n",
       " 'source_documents': [Document(id='fab1fd3c-ecf2-42dc-b4fe-5ba7b01f2d00', metadata={'author': '', 'creationDate': \"D:20231117152045-00'00'\", 'creationdate': '2023-11-17T15:20:45+00:00', 'creator': 'LaTeX with hyperref', 'file_path': 'files\\\\pumpkin_book.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': '', 'moddate': '', 'page': 1, 'producer': 'xdvipdfmx (20200315)', 'source': 'files\\\\pumpkin_book.pdf', 'subject': '', 'title': '', 'total_pages': 196, 'trapped': ''}, page_content='前言\\n“周志华老师的《机器学习》（西瓜书）是机器学习领域的经典入门教材之一，周老师为了使尽可能多的读\\n者通过西瓜书对机器学习有所了解, 所以在书中对部分公式的推导细节没有详述，但是这对那些想深究公式推\\n导细节的读者来说可能“不太友好”，本书旨在对西瓜书里比较难理解的公式加以解析，以及对部分公式补充\\n具体的推导细节。”\\n读到这里，大家可能会疑问为啥前面这段话加了引号，因为这只是我们最初的遐想，后来我们了解到，周\\n老师之所以省去这些推导细节的真实原因是，他本尊认为“理工科数学基础扎实点的大二下学生应该对西瓜书\\n中的推导细节无困难吧，要点在书里都有了，略去的细节应能脑补或做练习”。所以...... 本南瓜书只能算是我\\n等数学渣渣在自学的时候记下来的笔记，希望能够帮助大家都成为一名合格的“理工科数学基础扎实点的大二\\n下学生”。\\n使用说明\\n• 南瓜书的所有内容都是以西瓜书的内容为前置知识进行表述的，所以南瓜书的最佳使用方法是以西瓜书\\n为主线，遇到自己推导不出来或者看不懂的公式时再来查阅南瓜书；\\n• 对于初学机器学习的小白，西瓜书第1 章和第2 章的公式强烈不建议深究，简单过一下即可，等你学得'),\n",
       "  Document(id='5c9e0514-662d-48d7-a812-def2cdec960f', metadata={'author': '', 'creationDate': \"D:20231117152045-00'00'\", 'creationdate': '2023-11-17T15:20:45+00:00', 'creator': 'LaTeX with hyperref', 'file_path': 'files\\\\pumpkin_book.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': '', 'moddate': '', 'page': 1, 'producer': 'xdvipdfmx (20200315)', 'source': 'files\\\\pumpkin_book.pdf', 'subject': '', 'title': '', 'total_pages': 196, 'trapped': ''}, page_content='最新版PDF 获取地址：https://github.com/datawhalechina/pumpkin-book/releases\\n编委会\\n主编：Sm1les、archwalker、jbb0523\\n编委：juxiao、Majingmin、MrBigFan、shanry、Ye980226\\n封面设计：构思-Sm1les、创作-林王茂盛\\n致谢\\n特别感谢awyd234、feijuan、Ggmatch、Heitao5200、huaqing89、LongJH、LilRachel、LeoLRH、Nono17、\\nspareribs、sunchaothu、StevenLzq 在最早期的时候对南瓜书所做的贡献。\\n扫描下方二维码，然后回复关键词“南瓜书”，即可加入“南瓜书读者交流群”\\n版权声明\\n本作品采用知识共享署名-非商业性使用-相同方式共享4.0 国际许可协议进行许可。'),\n",
       "  Document(id='aa7c50b1-355b-4e5a-b26e-798fd4e9d601', metadata={'author': '', 'creationDate': \"D:20231117152045-00'00'\", 'creationdate': '2023-11-17T15:20:45+00:00', 'creator': 'LaTeX with hyperref', 'file_path': 'files\\\\pumpkin_book.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': '', 'moddate': '', 'page': 1, 'producer': 'xdvipdfmx (20200315)', 'source': 'files\\\\pumpkin_book.pdf', 'subject': '', 'title': '', 'total_pages': 196, 'trapped': ''}, page_content='• 对于初学机器学习的小白，西瓜书第1 章和第2 章的公式强烈不建议深究，简单过一下即可，等你学得\\n有点飘的时候再回来啃都来得及；\\n• 每个公式的解析和推导我们都力(zhi) 争(neng) 以本科数学基础的视角进行讲解，所以超纲的数学知识\\n我们通常都会以附录和参考文献的形式给出，感兴趣的同学可以继续沿着我们给的资料进行深入学习；\\n• 若南瓜书里没有你想要查阅的公式，或者你发现南瓜书哪个地方有错误，请毫不犹豫地去我们GitHub 的\\nIssues（地址：https://github.com/datawhalechina/pumpkin-book/issues）进行反馈，在对应版块\\n提交你希望补充的公式编号或者勘误信息，我们通常会在24 小时以内给您回复，超过24 小时未回复的\\n话可以微信联系我们（微信号：at-Sm1les）；\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n在线阅读地址：https://datawhalechina.github.io/pumpkin-book（仅供第1 版）'),\n",
       "  Document(id='3062179e-ad64-4f67-b3dd-96a72f4efbbb', metadata={'author': '', 'creationDate': \"D:20231117152045-00'00'\", 'creationdate': '2023-11-17T15:20:45+00:00', 'creator': 'LaTeX with hyperref', 'file_path': 'files\\\\pumpkin_book.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': '', 'moddate': '', 'page': 87, 'producer': 'xdvipdfmx (20200315)', 'source': 'files\\\\pumpkin_book.pdf', 'subject': '', 'title': '', 'total_pages': 196, 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n[2] 李航. 统计学习方法. 清华大学出版社, 2012.\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←')]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = qa_chain({\"query\": \"什么是南瓜书？\"})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "大模型+知识库后回答 question_2 的结果：\n",
      "Prompt Engineering for Developer是由吴恩达教授和Isa Fulford教授共同编写的。谢谢你的提问！\n"
     ]
    }
   ],
   "source": [
    "result = qa_chain({\"query\": \"Prompt Engineering for Developer是谁写的？\"})\n",
    "print(\"大模型+知识库后回答 question_2 的结果：\")\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AriZu\\AppData\\Local\\Temp\\ipykernel_18672\\924274261.py:5: LangChainDeprecationWarning: The method `BaseChatModel.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  llm.predict(prompt_template)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Prompt Engineering for Developer是由一位名叫Yufan Lu的作者撰写的。'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = \"\"\"请回答下列问题:\n",
    "                            {}\"\"\".format(\"Prompt Engineering for Developer是谁写的？\")\n",
    "\n",
    "### 基于大模型的问答\n",
    "llm.predict(prompt_template)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchstudy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
